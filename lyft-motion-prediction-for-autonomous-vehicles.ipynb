{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":19990,"databundleVersionId":1472735,"sourceType":"competition"},{"sourceId":1779841,"sourceType":"datasetVersion","datasetId":1058170},{"sourceId":43970252,"sourceType":"kernelVersion"},{"sourceId":50216082,"sourceType":"kernelVersion"},{"sourceId":50258316,"sourceType":"kernelVersion"},{"sourceId":50258373,"sourceType":"kernelVersion"}],"dockerImageVersionId":30043,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nLyft Motion Prediction for Autonomous Vehicles\n================================================\n\n本代码实现了基于 Lyft 数据集的运动预测任务，使用了 PyTorch Lightning 框架，\n并整合了 l5kit 库来处理数据集和场景栅格图像。代码中包含了数据加载、预处理、\n模型训练、验证和测试的完整流程，同时提供了详细的注释说明设计初衷及参数选择。\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imports and general settings","metadata":{}},{"cell_type":"code","source":"pip install timm==0.4.12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:10:30.967244Z","iopub.execute_input":"2025-04-16T04:10:30.967561Z","iopub.status.idle":"2025-04-16T04:10:37.890517Z","shell.execute_reply.started":"2025-04-16T04:10:30.967532Z","shell.execute_reply":"2025-04-16T04:10:37.889667Z"}},"outputs":[{"name":"stdout","text":"Collecting timm==0.4.12\n  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n\u001b[K     |████████████████████████████████| 376 kB 7.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (1.7.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (0.8.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.12) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.12) (3.7.4.1)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.12) (0.6)\nRequirement already satisfied: numpy in /kaggle/usr/lib/lyft-l5kit-unofficial-fix (from torch>=1.4->timm==0.4.12) (1.19.2)\nRequirement already satisfied: numpy in /kaggle/usr/lib/lyft-l5kit-unofficial-fix (from torch>=1.4->timm==0.4.12) (1.19.2)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (1.7.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.12) (8.0.1)\nInstalling collected packages: timm\nSuccessfully installed timm-0.4.12\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 24.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ---------------------------\n# Imports 以及全局设置\n# ---------------------------\nimport argparse    # 命令行参数解析模块\nimport os          # 操作系统交互模块（路径、环境变量等）\nimport random      # Python 内置随机数模块\nimport sys         # 访问Python解释器相关的变量\nfrom typing import Tuple  # 用于类型提示\n\nimport matplotlib.pyplot as plt  # 用于数据可视化\nimport numpy as np               # 数值计算模块\nimport pytorch_lightning as pl   # PyTorch Lightning 框架，用于简化训练过程\nimport torch                     # PyTorch 框架核心模块\nimport l5kit                     # L5Kit 提供数据集处理和栅格化功能（Kaggle环境下需预装）\nfrom l5kit.configs import load_config_data  # 用于加载配置信息（yaml格式）\nfrom l5kit.data import ChunkedDataset, LocalDataManager  # 数据集加载和管理工具\nfrom l5kit.dataset import AgentDataset  # 封装了代理（agent）数据集\nfrom l5kit.evaluation import compute_metrics_csv, write_pred_csv  # 评估指标计算和保存预测结果的工具\nfrom l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace  # 计算多模态损失和时间偏移误差\nfrom l5kit.geometry import transform_points  # 坐标转换工具\nfrom l5kit.rasterization import build_rasterizer  # 构建场景栅格图像的工具\nfrom l5kit.visualization import TARGET_POINTS_COLOR, draw_trajectory  # 用于绘制轨迹及设置目标点颜色\nfrom torch.utils.data import DataLoader  # PyTorch 数据加载工具\n\n# 自定义模块，用于定义损失函数、模型结构和一些工具函数\nimport lyft_loss\nimport lyft_models\nimport lyft_utils","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:10:40.227976Z","iopub.execute_input":"2025-04-16T04:10:40.228312Z","iopub.status.idle":"2025-04-16T04:10:45.171507Z","shell.execute_reply.started":"2025-04-16T04:10:40.228277Z","shell.execute_reply":"2025-04-16T04:10:45.170734Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 定义数据集规模和验证时采样间隔的常数\nALL_DATA_SIZE = 198474478\nVAL_INTERVAL_SAMPLES = 250000\n\n# 定义栅格化配置文件路径，该文件中包含了场景渲染需要的各项配置参数\nCFG_PATH = \"../input/lyft-mpred-seresnext26-pretrained/agent_motion_config.yaml\"\n# 预测结果输出的CSV文件路径，后续测试模式会生成该文件\nCSV_PATH = \"./submission.csv\"\n\n# 为保证训练和测试环境的一致性，设置历史帧和未来帧的阈值\nMIN_FRAME_HISTORY = 0    # 筛选代理：过去至少需要的帧数\nMIN_FRAME_FUTURE = 10    # 筛选代理：未来至少需要的帧数\n# 验证集中用到的特定帧（比如帧99）用来与测试数据保持一致\nVAL_SELECTED_FRAME = (99,)\n\n# ---------------------------\n# 固定随机种子设置，确保实验结果的可重复性\n# ---------------------------\nSEED = 42\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)  # Python 的hash随机化的种子\nrandom.seed(SEED)  # Python自带随机种子\nnp.random.seed(SEED)  # NumPy随机种子","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:10:48.136030Z","iopub.execute_input":"2025-04-16T04:10:48.136362Z","iopub.status.idle":"2025-04-16T04:10:48.142148Z","shell.execute_reply.started":"2025-04-16T04:10:48.136327Z","shell.execute_reply":"2025-04-16T04:10:48.141181Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Run configuration","metadata":{}},{"cell_type":"code","source":"# ---------------------------\n# 加载配置文件\n# ---------------------------\ncfg = load_config_data(CFG_PATH)  # 从yaml配置文件中加载配置信息\n\n# ---------------------------\n# 参数解析与运行配置\n# ---------------------------\nparser = argparse.ArgumentParser(\n    description=\"Run lyft motion prediction learning\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\n# 定义数据集路径\nparser.add_argument(\n    \"--l5kit_data_folder\",\n    default=\"/your/dataset/path\",\n    type=str,\n    help=\"root directory path for lyft motion prediction dataset\",\n)\n\n# 定义优化器选择，可选 \"adam\" 或 \"sgd\"\nparser.add_argument(\n    \"--optim_name\",\n    choices=[\"adam\", \"sgd\"],\n    default=\"sgd\",\n    help=\"optimizer name\",\n)\n\n# 定义预测模式数（多模态预测）\nparser.add_argument(\n    \"--num_modes\",\n    type=int,\n    default=3,\n    help=\"number of the modes on each prediction\",\n)\n\n# 定义学习率\nparser.add_argument(\"--lr\", default=7.0e-4, type=float, help=\"learning rate\")\n\n# 定义批次大小\nparser.add_argument(\"--batch_size\", type=int, default=220, help=\"batch size\")\n\n# 定义训练轮数\nparser.add_argument(\"--epochs\", type=int, default=1, help=\"epochs for training\")\n\n# 定义网络骨干结构，可选 efficientnet_b1 或 seresnext26d_32x4d\nparser.add_argument(\n    \"--backbone_name\",\n    choices=[\"efficientnet_b1\", \"seresnext26d_32x4d\"],\n    default=\"seresnext26d_32x4d\",\n    help=\"backbone name\",\n)\n\n# 选择是否在训练时只使用部分帧（4帧），加快收敛但会增加损失值\nparser.add_argument(\n    \"--downsample_train\",\n    action=\"store_true\",\n    help=\"using only 4 frames from each scene, the loss converge is \\\nmuch faster than using all data, but it will get larger loss\",\n)\n\n# 测试模式的标记，用于区分训练和测试阶段\nparser.add_argument(\"--is_test\", action=\"store_true\", help=\"test mode\")\n\n# 模型检查点路径，用于测试模式加载预训练模型权重\nparser.add_argument(\n    \"--ckpt_path\",\n    type=str,\n    default=\"./model.pth\",\n    help=\"path for model checkpoint at test mode\",\n)\n\n# 定义训练时的数值精度\nparser.add_argument(\n    \"--precision\",\n    default=16,\n    choices=[16, 32],\n    type=int,\n    help=\"float precision at training\",\n)\n\n# 指定使用的GPU设备编号，逗号分隔，如 \"0\" 或 \"0,1\"\nparser.add_argument(\n    \"--visible_gpus\",\n    type=str,\n    default=\"0\",\n    help=\"Select gpu ids with comma separated format\",\n)\n\n# 是否启用自动寻找学习率（如 FastAI 实现）\nparser.add_argument(\n    \"--find_lr\",\n    action=\"store_true\",\n    help=\"find lr with fast ai implementation\",\n)\n\n# 定义 DataLoader 使用的 CPU 核数\nparser.add_argument(\n    \"--num_workers\",\n    default=\"16\",\n    type=int,\n    help=\"number of cpus for DataLoader\",\n)\n\n# 调试模式的标记，用于快速验证代码运行而不消耗全部数据\nparser.add_argument(\"--is_debug\", action=\"store_true\", help=\"debug mode\")\n\n# 为了解决笔记本或者Kaggle环境下不能通过命令行传参的问题，这里直接模拟传参\nargs = parser.parse_args([\n    \"--l5kit_data_folder\",\n    \"../input/lyft-motion-prediction-autonomous-vehicles\",\n    \"--is_test\",\n    \"--ckpt_path\",\n    \"../input/lyft-mpred-seresnext26-pretrained/epoch-v0.ckpt\",\n    \"--num_workers\",\n    \"4\",\n    \"--batch_size\",\n    \"32\"\n])\n\n# 根据debug标识调整一些参数，比如数据集、采样间隔、batch_size等\nif args.is_debug:\n    DEBUG = True\n    print(\"\\t ---- DEBUG RUN ---- \")\n    cfg[\"train_data_loader\"][\"key\"] = \"scenes/sample.zarr\"\n    cfg[\"val_data_loader\"][\"key\"] = \"scenes/sample.zarr\"\n    VAL_INTERVAL_SAMPLES = 5000\n    args.batch_size = 16\nelse:\n    DEBUG = False\n    print(\"\\t ---- NORMAL RUN ---- \")\n\n# 输出当前的所有参数，方便调试和验证选择的配置\nlyft_utils.print_argparse_arguments(args)\n\n# 设置 GPU 可见性，确保程序仅使用指定的 GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:10:50.147555Z","iopub.execute_input":"2025-04-16T04:10:50.147908Z","iopub.status.idle":"2025-04-16T04:10:50.173374Z","shell.execute_reply.started":"2025-04-16T04:10:50.147874Z","shell.execute_reply":"2025-04-16T04:10:50.172664Z"}},"outputs":[{"name":"stdout","text":"\t ---- NORMAL RUN ---- \nPARAMETER SETTING\n--------------------------------------------------\nbackbone_name            :seresnext26d_32x4d\nbatch_size               :32\nckpt_path                :../input/lyft-mpred-seresnext26-pretrained/epoch-v0.ckpt\ndownsample_train         :False\nepochs                   :1\nfind_lr                  :False\nis_debug                 :False\nis_test                  :True\nl5kit_data_folder        :../input/lyft-motion-prediction-autonomous-vehicles\nlr                       :0.0007\nnum_modes                :3\nnum_workers              :4\noptim_name               :sgd\nprecision                :16\nvisible_gpus             :0\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Dataloader preparation\n`pl.LightningDataModule` for train, validation and test dataloader.","metadata":{}},{"cell_type":"code","source":"# ---------------------------\n# 数据加载模块：LyftMpredDatamodule (继承自 pl.LightningDataModule)\n# ---------------------------\nclass LyftMpredDatamodule(pl.LightningDataModule):\n    \"\"\"\n    该数据模块封装了训练、验证和测试数据集的加载逻辑。包含数据准备、数据集分割和可视化的功能。\n    \"\"\"\n    def __init__(\n        self,\n        l5kit_data_folder: str,\n        cfg: dict,\n        batch_size: int = 440,\n        num_workers: int = 16,\n        downsample_train: bool = False,\n        is_test: bool = False,\n        is_debug: bool = False,\n    ) -> None:\n        super().__init__()\n        # 设置数据存放的根目录（通过环境变量设置，l5kit库会自动读取）\n        os.environ[\"L5KIT_DATA_FOLDER\"] = l5kit_data_folder\n        self.cfg = cfg\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.downsample_train = downsample_train\n        self.is_test = is_test\n        self.is_debug = is_debug\n\n    def prepare_data(self):\n        \"\"\"\n        仅在单个 GPU 上调用，用于下载和预处理数据。\n        这里主要初始化本地数据管理器和构建场景栅格化器。\n        \"\"\"\n        self.dm = LocalDataManager(None)\n        self.rasterizer = build_rasterizer(cfg, self.dm)\n\n    def setup(self):\n        \"\"\"\n        在每个 GPU 上调用，用于实际加载和处理数据集。\n        根据 is_test 标志加载测试或者训练/验证数据集，\n        并对数据集做必要的下采样和可视化处理。\n        \"\"\"\n        if self.is_test:\n            print(\"test mode setup\")\n            self.test_path, test_zarr, self.test_dataset = self.load_zarr_dataset(\n                loader_name=\"test_data_loader\"\n            )\n        else:\n            print(\"train mode setup\")\n            # 加载训练数据集\n            self.train_path, train_zarr, self.train_dataset = self.load_zarr_dataset(\n                loader_name=\"train_data_loader\"\n            )\n            # 加载验证数据集\n            self.val_path, val_zarr, self.val_dataset = self.load_zarr_dataset(\n                loader_name=\"val_data_loader\"\n            )\n            # 绘制部分训练数据用于直观检查数据加载情况及渲染效果\n            self.plot_dataset(self.train_dataset)\n\n            # 如果设置了 downsample_train，则仅选取部分帧以加速训练\n            if self.downsample_train:\n                print(\n                    \"downsampling agents, using only {} frames from each scene\".format(\n                        len(lyft_utils.TRAIN_DSAMPLE_FRAMES)\n                    )\n                )\n                train_agents_list = lyft_utils.downsample_agents(\n                    train_zarr,\n                    self.train_dataset,\n                    selected_frames=lyft_utils.TRAIN_DSAMPLE_FRAMES,\n                )\n                self.train_dataset = torch.utils.data.Subset(\n                    self.train_dataset, train_agents_list\n                )\n            # 对验证数据集下采样，保证与测试集采样一致（可以参考 l5kit.evaluation.create_chopped_dataset）\n            val_agents_list = lyft_utils.downsample_agents(\n                val_zarr, self.val_dataset, selected_frames=VAL_SELECTED_FRAME\n            )\n            self.val_dataset = torch.utils.data.Subset(\n                self.val_dataset, val_agents_list\n            )\n\n    def train_dataloader(self):\n        \"\"\"\n        返回训练阶段用到的数据加载器，采用随机打乱（shuffle=True）。\n        \"\"\"\n        return DataLoader(\n            self.train_dataset,\n            shuffle=True,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n        )\n\n    def val_dataloader(self):\n        \"\"\"\n        返回验证阶段用到的数据加载器，不进行随机打乱。\n        \"\"\"\n        return DataLoader(\n            self.val_dataset,\n            shuffle=False,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n        )\n\n    def test_dataloader(self):\n        \"\"\"\n        返回测试阶段用到的数据加载器，不进行随机打乱。\n        \"\"\"\n        return DataLoader(\n            self.test_dataset,\n            shuffle=False,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n        )\n\n    def load_zarr_dataset(\n        self, loader_name: str = \"train_data_loder\"\n    ) -> Tuple[str, ChunkedDataset, AgentDataset]:\n        \"\"\"\n        加载 zarr 格式的序列化数据集，并根据 loader_name 选择不同的数据预处理方式。\n        对于测试数据还会加载对应的 mask 文件，用于标识哪些代理数据可用。\n        \"\"\"\n        zarr_path = self.dm.require(self.cfg[loader_name][\"key\"])\n        print(\"load zarr data:\", zarr_path)\n        # 打开 zarr 数据集（分块存储，提升加载效率）\n        zarr_dataset = ChunkedDataset(zarr_path).open()\n        if loader_name == \"test_data_loader\":\n            # 对于测试数据，加载场景中代理的 mask（有效性标识）\n            mask_path = os.path.join(os.path.dirname(zarr_path), \"mask.npz\")\n            agents_mask = np.load(mask_path)[\"arr_0\"]\n            agent_dataset = AgentDataset(\n                self.cfg, zarr_dataset, self.rasterizer, agents_mask=agents_mask\n            )\n        else:\n            # 对于训练和验证数据，使用 min_frame_history 和 min_frame_future 进行筛选\n            agent_dataset = AgentDataset(\n                self.cfg,\n                zarr_dataset,\n                self.rasterizer,\n                min_frame_history=MIN_FRAME_HISTORY,\n                min_frame_future=MIN_FRAME_FUTURE,\n            )\n        print(zarr_dataset)\n        return zarr_path, zarr_dataset, agent_dataset\n\n    def plot_dataset(self, agent_dataset: AgentDataset, plot_num: int = 10) -> None:\n        \"\"\"\n        随机挑选若干个样本，生成栅格图像，叠加目标轨迹，用于直观验证数据渲染效果。\n        如果处于调试模式下，可以通过 plt.show() 展示图像。\n        \"\"\"\n        print(\"Ploting dataset\")\n        # 随机选取 plot_num 个样本\n        ind = np.random.randint(0, len(agent_dataset), size=plot_num)\n        for i in range(plot_num):\n            data = agent_dataset[ind[i]]\n            # 将数据中的图像张量格式转换为 (H, W, C) 以便可视化\n            im = data[\"image\"].transpose(1, 2, 0)\n            # 利用栅格化器将图像转换成 RGB 格式\n            im = agent_dataset.rasterizer.to_rgb(im)\n            # 将目标轨迹点从代理坐标转换为像素坐标\n            target_positions_pixels = transform_points(\n                data[\"target_positions\"], data[\"raster_from_agent\"]\n            )\n            # 绘制目标轨迹（使用预定义的颜色）\n            draw_trajectory(\n                im,\n                target_positions_pixels,\n                TARGET_POINTS_COLOR,\n                yaws=data[\"target_yaws\"],\n            )\n            # 反转图像的 Y 轴进行正确显示\n            plt.imshow(im[::-1])\n            if self.is_debug:\n                plt.show()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:10:53.167682Z","iopub.execute_input":"2025-04-16T04:10:53.167965Z","iopub.status.idle":"2025-04-16T04:10:53.188174Z","shell.execute_reply.started":"2025-04-16T04:10:53.167941Z","shell.execute_reply":"2025-04-16T04:10:53.187246Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Train, validation and test steps\n with `pl.LightningModule`.","metadata":{}},{"cell_type":"code","source":"# ---------------------------\n# 模型训练模块：LightningModule 封装了训练、验证及测试步骤\n# ---------------------------\nclass LitModel(pl.LightningModule):\n    \"\"\"\n    继承自 PyTorch LightningModule，封装了前向计算、训练、验证和测试流程。\n    同时负责配置优化器和学习率调度策略。\n    \"\"\"\n    def __init__(\n        self,\n        cfg: dict,\n        num_modes: int = 3,\n        ba_size: int = 128,\n        lr: float = 3.0e-4,\n        backbone_name: str = \"efficientnet_b1\",\n        epochs: int = 1,\n        total_steps: int = 100,\n        data_size: int = ALL_DATA_SIZE,\n        optim_name: str = \"adam\",\n    ) -> None:\n        super().__init__()\n        # 保存所有超参数，方便日志记录和后续恢复\n        self.save_hyperparameters(\n            \"lr\",\n            \"backbone_name\",\n            \"num_modes\",\n            \"ba_size\",\n            \"epochs\",\n            \"optim_name\",\n            \"data_size\",\n            \"total_steps\",\n        )\n        # 使用自定义模型构造函数，构建多模态预测模型\n        self.model = lyft_models.LyftMultiModel(\n            cfg, num_modes=num_modes, backbone_name=backbone_name\n        )\n        # 测试阶段需要用到的关键数据字段\n        self.test_keys = (\"world_from_agent\", \"centroid\", \"timestamp\", \"track_id\")\n\n    def forward(self, x):\n        \"\"\"\n        前向传播函数，直接调用内部的模型。\n        \"\"\"\n        x = self.model(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        \"\"\"\n        单个训练批次的处理逻辑：\n        1. 获取输入图像及对应的目标轨迹和可用性标记。\n        2. 前向传播获得预测值及置信度。\n        3. 利用自定义损失函数计算负多模态对数似然损失。\n        4. 记录并返回损失值以供反向传播。\n        \"\"\"\n        inputs = batch[\"image\"]\n        target_availabilities = batch[\"target_availabilities\"].unsqueeze(-1)\n        targets = batch[\"target_positions\"]\n\n        outputs, confidences = self.model(inputs)\n        loss = lyft_loss.pytorch_neg_multi_log_likelihood_batch(\n            targets,\n            outputs,\n            confidences.squeeze(),\n            target_availabilities.squeeze(),\n        )\n        self.log(\n            \"train_epoch_loss\",\n            loss,\n            prog_bar=False,\n            on_epoch=True,\n            on_step=False,\n        )\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        \"\"\"\n        与训练步骤类似，验证时计算损失值用于监控模型表现，调参时也作为早停的依据。\n        \"\"\"\n        inputs = batch[\"image\"]\n        target_availabilities = batch[\"target_availabilities\"].unsqueeze(-1)\n        targets = batch[\"target_positions\"]\n\n        outputs, confidences = self.model(inputs)\n        loss = lyft_loss.pytorch_neg_multi_log_likelihood_batch(\n            targets,\n            outputs,\n            confidences.squeeze(),\n            target_availabilities.squeeze(),\n        )\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        \"\"\"\n        测试步骤：\n        1. 前向传播获得模型预测的轨迹及对应置信度。\n        2. 同时保留批次中的其他关键信息（如世界坐标转换矩阵、车体中心等）。\n        这些信息将在 test_epoch_end 中进一步转换处理并保存为 CSV 文件。\n        \"\"\"\n        inputs = batch[\"image\"]\n        outputs, confidences = self.model(inputs)\n        test_batch = {key_: batch[key_] for key_ in self.test_keys}\n\n        return outputs, confidences, test_batch\n\n    def test_epoch_end(self, outputs):\n        \"\"\"\n        测试阶段所有批次结束后：\n        1. 将批次结果中的预测轨迹转换回世界坐标，计算相对偏移。\n        2. 整合所有批次的预测结果、置信度、时间戳和轨迹ID。\n        3. 调用 write_pred_csv 函数，将预测结果保存为 CSV 文件以供提交评估。\n        \"\"\"\n        pred_coords_list = []\n        confidences_list = []\n        timestamps_list = []\n        track_id_list = []\n\n        # 对所有批次输出结果进行处理\n        for outputs, confidences, batch in outputs:\n            # 将 tensor 数据转换为 numpy 格式\n            outputs = outputs.cpu().numpy()\n\n            world_from_agents = batch[\"world_from_agent\"].cpu().numpy()\n            centroids = batch[\"centroid\"].cpu().numpy()\n            # 对每个样本和每个模态，转换预测结果到世界坐标，并调整为相对于车体中心的坐标\n            for idx in range(len(outputs)):\n                for mode in range(3):\n                    outputs[idx, mode, :, :] = (\n                        transform_points(\n                            outputs[idx, mode, :, :], world_from_agents[idx]\n                        )\n                        - centroids[idx][:2]\n                    )\n            pred_coords_list.append(outputs)\n            confidences_list.append(confidences)\n            timestamps_list.append(batch[\"timestamp\"])\n            track_id_list.append(batch[\"track_id\"])\n\n        # 将多个批次的预测结果拼接起来\n        coords = np.concatenate(pred_coords_list)\n        confs = torch.cat(confidences_list).cpu().numpy()\n        track_ids = torch.cat(track_id_list).cpu().numpy()\n        timestamps = torch.cat(timestamps_list).cpu().numpy()\n\n        # 将测试预测结果写入 CSV 文件\n        write_pred_csv(\n            CSV_PATH,\n            timestamps=timestamps,\n            track_ids=track_ids,\n            coords=coords,\n            confs=confs,\n        )\n        print(f\"Saved to {CSV_PATH}\")\n\n    def configure_optimizers(self):\n        \"\"\"\n        根据命令行选择的优化器类型，配置相应的优化器（SGD 或 Adam）\n        并采用 OneCycleLR 调度器控制学习率变化。\n        \"\"\"\n        if self.hparams.optim_name == \"sgd\":\n            optimizer = torch.optim.SGD(\n                self.parameters(),\n                lr=self.hparams.lr,\n                momentum=0.9,\n                weight_decay=4e-5,\n            )\n        elif self.hparams.optim_name == \"adam\":\n            optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n        else:\n            raise NotImplementedError\n\n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer, max_lr=self.hparams.lr, total_steps=self.hparams.total_steps\n        )\n        return [optimizer], [scheduler]","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:10:56.175555Z","iopub.execute_input":"2025-04-16T04:10:56.175888Z","iopub.status.idle":"2025-04-16T04:10:56.196566Z","shell.execute_reply.started":"2025-04-16T04:10:56.175854Z","shell.execute_reply":"2025-04-16T04:10:56.195630Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ---------------------------\n# 实例化数据模块并准备数据\n# ---------------------------\n# 使用命令行参数配置，创建数据模块对象，该对象负责加载训练、验证和测试数据\nmpred_dm = LyftMpredDatamodule(  # type: ignore[abstract]\n    args.l5kit_data_folder,\n    cfg,\n    batch_size=args.batch_size,\n    num_workers=args.num_workers,\n    downsample_train=args.downsample_train,\n    is_test=args.is_test,\n    is_debug=args.is_debug,\n)\nmpred_dm.prepare_data()  # 预处理数据，仅在单个 GPU 上运行\nmpred_dm.setup()         # 配置数据集（加载文件、下采样、可视化等）","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:10:59.359418Z","iopub.execute_input":"2025-04-16T04:10:59.359717Z","iopub.status.idle":"2025-04-16T04:11:23.421943Z","shell.execute_reply.started":"2025-04-16T04:10:59.359693Z","shell.execute_reply":"2025-04-16T04:11:23.421138Z"}},"outputs":[{"name":"stdout","text":"test mode setup\nload zarr data: ../input/lyft-motion-prediction-autonomous-vehicles/scenes/test.zarr\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:101: RuntimeWarning: you're running with a custom agents_mask\n","output_type":"stream"},{"name":"stdout","text":"+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n|   11314    |  1131400   |  88594921  |    7854144    |      31.43      |        100.00        |        78.31         |        10.00         |        10.00        |\n+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Running test ","metadata":{}},{"cell_type":"code","source":"# ---------------------------\n# 模型测试流程\n# ---------------------------\nif args.is_test:\n    print(\"\\t\\t ==== TEST MODE ====\")\n    print(\"load from: \", args.ckpt_path)\n    # 从给定的检查点路径加载模型权重，构建模型实例\n    model = LitModel.load_from_checkpoint(args.ckpt_path, cfg=cfg)\n    # 使用指定的 GPU 数量创建 Trainer 对象\n    trainer = pl.Trainer(gpus=len(args.visible_gpus.split(\",\")))\n    # 执行测试流程，内部调用 test_dataloader、test_step、test_epoch_end 等方法\n    trainer.test(model, datamodule=mpred_dm)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:14:01.011097Z","iopub.execute_input":"2025-04-16T04:14:01.011539Z","iopub.status.idle":"2025-04-16T04:55:50.060058Z","shell.execute_reply.started":"2025-04-16T04:14:01.011499Z","shell.execute_reply":"2025-04-16T04:55:50.059076Z"}},"outputs":[{"name":"stdout","text":"\t\t ==== TEST MODE ====\nload from:  ../input/lyft-mpred-seresnext26-pretrained/epoch-v0.ckpt\n","output_type":"stream"},{"name":"stderr","text":"GPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02be7fcafe3f46e98967037feff10fed"}},"metadata":{}},{"name":"stdout","text":"Saved to ./submission.csv\n--------------------------------------------------------------------------------\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Running train","metadata":{}},{"cell_type":"code","source":"if not args.is_test:\n    print(\"\\t\\t ==== TRAIN MODE ====\")\n    print(\n        \"training samples: {}, valid samples: {}\".format(\n            len(mpred_dm.train_dataset), len(mpred_dm.val_dataset)\n        )\n    )\n    # 根据数据集的大小和批次数计算训练总步数，以及每个间隔内的验证步数\n    total_steps = args.epochs * len(mpred_dm.train_dataset) // args.batch_size\n    val_check_interval = VAL_INTERVAL_SAMPLES // args.batch_size\n\n    # 实例化 LightningModule，构造模型以及保存超参数信息\n    model = LitModel(\n        cfg,\n        lr=args.lr,\n        backbone_name=args.backbone_name,\n        num_modes=args.num_modes,\n        optim_name=args.optim_name,\n        ba_size=args.batch_size,\n        epochs=args.epochs,\n        data_size=len(mpred_dm.train_dataset),\n        total_steps=total_steps,\n    )\n\n    # 定义模型检查点回调，监控验证损失，并保存在验证集上表现最好的模型状态\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        monitor=\"val_loss\",\n        save_last=True,\n        mode=\"min\",\n        verbose=True,\n    )\n    # 固定整体种子，确保多GPU训练中各个进程的随机性一致\n    pl.trainer.seed_everything(seed=SEED)\n    # 构造 Trainer 对象，包含训练的 GPU 数量、最大步数、验证间隔、精度选项和检查点回调\n    trainer = pl.Trainer(\n        gpus=len(args.visible_gpus.split(\",\")),\n        max_steps=total_steps,\n        val_check_interval=val_check_interval,\n        precision=args.precision,\n        benchmark=True,\n        deterministic=False,\n        checkpoint_callback=checkpoint_callback,\n    )\n    # 开始模型训练\n    trainer.fit(model, datamodule=mpred_dm)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:23:13.676368Z","iopub.execute_input":"2025-04-16T05:23:13.676715Z","iopub.status.idle":"2025-04-16T05:23:13.685077Z","shell.execute_reply.started":"2025-04-16T05:23:13.676684Z","shell.execute_reply":"2025-04-16T05:23:13.684274Z"}},"outputs":[],"execution_count":15}]}